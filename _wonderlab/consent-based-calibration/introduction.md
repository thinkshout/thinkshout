---
index: 0
layout: wonderlab_blog
body-class: wonderlab-blog-post wonderlab-page
category: consent-based-calibration
title: "Introducing Consent-Based Calibration"
card-subheader:
author: kate
published: true
short: "Collecting and maintaining user data just got a lot more risky and a lot more expensive. It's time to shift the conversation from protecting user data to respecting your privacy. That's where consent-based calibration comes in."
description: "What is consent-based calibration? An alternative approach to personalization for nonprofits and social-change organizations."
tags:
- Consent-Based Calibration
- Personalization for Non-profits
- User-Centered Content Strategy
- Ethical Data Policy
- Optimization
- Engagement Model
date: 2019-01-22 09:00:00
image: https://thinkshout.com/assets/images/wonderlab/consent-based-collaboration/card/cbc-intro-card.jpg
image-alt: "Mirrored profile of a women"
header-image: /assets/images/wonderlab/consent-based-collaboration/hero/cbc-intro.jpg
header-image-alt: "Mirrored profile of a women"
caption: "Illustration by Sarah Leigh"
card-image: /assets/images/wonderlab/consent-based-collaboration/card/cbc-intro-card.jpg
card-image-alt: "Mirrored profile of a women"
medium-link: https://medium.com
---

Collecting and maintaining user data just got a lot more risky, and potentially, a lot more expensive.

This month’s implementation of California’s Consumer Privacy Act (CCPA) has a lot of organizations hustling to comply with the most rigorous data privacy regulation enacted in the U.S. to date. The CCPA demands a wide swath of participation, requiring companies and organizations (serving California residents) that meet one or all of the following criteria comply:

(1) Companies and organizations that have annual revenues of $25 million or (2) that have collected 50,000 or more personal data records or (3) that earn more than 50 percent of their income from the sale of personal data.

[Compliance looks like the usual stakes](https://oag.ca.gov/privacy/ccpa), and isn’t where the implementation burden actually falls. What’s critical about the CCPA is that it has some teeth in terms of user protections. Even if an organization doesn’t experience a data breach, organizations are at risk of being sued. That’s because the new law states that once a user has requested their personal data report, an organization has 30 days to deliver the user’s record, and if it fails to do so, users can receive up to $750 in compensation (or actual damages, whichever is higher) from the organization that has failed to comply. What’s more, in addition, the organization can experience a fine of $7,500 per personal record.

But here’s the thing: As a digital agency responsible for presenting the best possible set of solutions, our first question in response to CCPA isn’t just How can we help our clients comply? It’s _What is the best way to balance consumer privacy with content or product delivery?_

That’s where Consent-Based Calibration comes in, a new approach that addresses concerns about data privacy, unwieldy personalization software, and content-dense websites—and delights users at the same time. Consent-based calibration shifts our main concern from collecting and protecting user data to actually respecting your privacy.

>Consent-based calibration shifts our main concern from collecting and protecting user data to actually respecting your privacy.

Consent-based calibration is built on one simple idea and two core concepts:

## The Idea:

### We don’t need to know exactly who you are to serve your needs—or ours.

Organizations don’t need to know exactly who you are on a personal level in order to engage you in a meaningful way. And in fact, the basic tenets of personalization fail in ways that [make the benefits unworthy of the risks](https://www.nytimes.com/2019/11/05/opinion/personalization-privacy.html). Consent-based calibration asserts that it’s more respectful, impactful, and impressive if an organization actually doesn’t know your every click and can still serve up a deeply meaningful experience. Out of this idea come consent based calibration’s two central concepts:

## Concept 1:

### Replacing personal experiences with optional personas isn’t just ethical, it’s effective.

Instead of entering a digital platform and consenting to GDPR data collection, a user would enter the same digital platform and be offered a choice of experiences: How would you like to experience our site? We can offer a choice of experiences to you, and you can shift between them at any time. Instead of collecting data on the user specifically to tailor every next step, consent based calibration suggests next steps based on the persona rather than the person, and at key checkpoints, you can opt into a different persona, or a different experience.

One of the most widely agreed upon issues with personalization—and with AI in general—is that [it lacks the ability to understand that a user may want to radically change their course or interests](https://medium.com/inclusive-software/describing-personas-af992e3fc527): It fails to introduce us to the parts of ourselves that are unknown. Consent based calibration assumes that a person would like to actively participate in creating their own experience. It fosters respect by offering the choice to change that experience at any time.
And ultimately, it shifts the now-ubiquitous single prompt of  “Do you consent to us collecting your data to provide you with an experience that we define for you behind the scenes?” to an ongoing conversation of “Are you interested in interacting with our site in one of these typical user patterns? How about now?”

This shift from single ask—which gives the organization all of the decision-making power moving forward—to an ongoing consent-based calibration process—sows respect between organization and user. “We don’t need to know exactly who you are in order to share with you what you might be looking for” is the kind of consent-based mutuality that could only be developed by an organization that isn’t driven by commodification.

>Applying consent-based calibration would address many of the ethical and practical issues organizations and individual users face regarding data, content, and user experience. It refuses to accept that personalization means a more personal experience, and instead, calls it out for what it is: A commodification engine.

## Concept 2:

### Massive content management systems should be replaced with lightweight, interactive experiences that rely on browser-side data storage.

We’ve worked with hundreds of clients, and across all of them, one rule tends to be true: Less than half (at most) of an organization’s content drives 99 percent of its website’s meaningful engagement. Having run hundreds of analytics audits and discovery workshops with leaders across industries, we can say that everyone shares a similar burden: too much content, not enough meaning, not enough engagement. Critically, the data collection required for personalization only doubles down on that burden: Massive amounts of content generate massive amounts of data—and massive amounts of data make both organizations increasingly vulnerable to data breaches and users vulnerable to repetitive personalization experiences.

Applying consent-based calibration would address many of the ethical and practical issues organizations and individual users face regarding data, content, and user experience. It refuses to accept that personalization means a more personal experience, and instead, calls it out for what it is: A commodification engine.

Organizations doing good in the world, the kinds of clients that ThinkShout serves, don’t need to leverage tools developed for extraction in order to deliver experiences meant for action. Consent-based calibration is one simple response to the need for organizations to create experiences for their users to feel understood and mutually respected in the digital. We know it's possible, now we just need to do the work of making ideas like these probable.
